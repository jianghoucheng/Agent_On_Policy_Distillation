- algorithm.adv_estimator=grpo
- algorithm.use_kl_in_reward=False
- algorithm.kl_ctrl.kl_coef=0.0
- data.train_files=['/data0/user10/Open-AgentRL/dataset/Gen-Verse/Open-AgentRL-30K/Open-AgentRL-30K.parquet']
- data.val_files=['/data0/user10/Open-AgentRL/dataset/Gen-Verse/Open-AgentRL-Eval/aime2025/aime_2025_problems.parquet',
  '/data0/user10/Open-AgentRL/dataset/Gen-Verse/Open-AgentRL-Eval/aime2024/aime_2024_problems.parquet']
- data.return_raw_chat=True
- data.train_batch_size=32
- data.max_prompt_length=2048
- data.max_response_length=8096
- data.prompt_key=prompt
- data.filter_overlong_prompts=True
- data.truncation=error
- data.custom_cls.path=recipe/demystify/reward.py
- data.custom_cls.name=CustomRLHFDataset
- custom_reward_function.path=recipe/demystify/reward.py
- custom_reward_function.name=compute_score
- actor_rollout_ref.model.path=/data0/user10/Open-AgentRL/checkpoint/Qwen3-4B-RA-SFT
- actor_rollout_ref.model.use_remove_padding=True
- actor_rollout_ref.model.enable_gradient_checkpointing=True
- actor_rollout_ref.actor.use_kl_loss=False
- actor_rollout_ref.actor.kl_loss_coef=0.0
- actor_rollout_ref.actor.clip_ratio_low=0.2
- actor_rollout_ref.actor.clip_ratio_high=0.28
- actor_rollout_ref.actor.grad_clip=1.0
- actor_rollout_ref.actor.clip_ratio_c=10.0
- actor_rollout_ref.actor.loss_agg_mode=token-mean
- actor_rollout_ref.actor.optim.lr=1e-6
- actor_rollout_ref.actor.use_dynamic_bsz=True
- actor_rollout_ref.actor.ppo_mini_batch_size=8
- actor_rollout_ref.actor.ppo_max_token_len_per_gpu=10144
- actor_rollout_ref.actor.ulysses_sequence_parallel_size=4
- actor_rollout_ref.actor.fsdp_config.param_offload=True
- actor_rollout_ref.actor.fsdp_config.optimizer_offload=True
- actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=40576
- actor_rollout_ref.rollout.name=vllm
- actor_rollout_ref.rollout.mode=async
- actor_rollout_ref.rollout.tensor_model_parallel_size=4
- actor_rollout_ref.rollout.multi_turn.enable=True
- actor_rollout_ref.rollout.multi_turn.max_user_turns=16
- actor_rollout_ref.rollout.multi_turn.max_assistant_turns=16
- actor_rollout_ref.rollout.multi_turn.tool_config_path=recipe/demystify/sandbox_fusion_tool_config.yaml
- actor_rollout_ref.rollout.multi_turn.format=hermes
- actor_rollout_ref.rollout.gpu_memory_utilization=0.5
- actor_rollout_ref.rollout.n=16
- actor_rollout_ref.rollout.val_kwargs.top_p=0.6
- actor_rollout_ref.rollout.val_kwargs.temperature=1.0
- actor_rollout_ref.rollout.val_kwargs.n=32
- reward_model.reward_manager=dapo
- +reward_model.reward_kwargs.overlong_buffer_cfg.enable=True
- +reward_model.reward_kwargs.overlong_buffer_cfg.len=1024
- +reward_model.reward_kwargs.overlong_buffer_cfg.penalty_factor=1.0
- +reward_model.reward_kwargs.overlong_buffer_cfg.log=false
- +reward_model.reward_kwargs.max_resp_len=8096
- trainer.logger=[console,wandb]
- trainer.project_name=demystify-agentic-rl
- trainer.experiment_name=GRPO-TCR-Qwen3-4B
- trainer.n_gpus_per_node=4
- trainer.val_before_train=False
- trainer.log_val_generations=20
- trainer.nnodes=1
- trainer.save_freq=30
- trainer.default_local_dir=/data0/user10/Open-AgentRL/checkpoint/GRPO-TCR-Qwen3-4B
- trainer.test_freq=10
- trainer.total_epochs=3
